"""Fix Sankey files not passing validation

Revision ID: e89de4f6cd7f
Revises: 749576d6afe2
Create Date: 2021-11-08 13:21:47.454052

"""
import hashlib
import json
from os import path

import fastjsonschema
import sqlalchemy as sa
from alembic import context
from alembic import op
from sqlalchemy import table, column, and_
from sqlalchemy.orm import Session

from migrations.utils import window_chunk

# revision identifiers, used by Alembic.
revision = 'e89de4f6cd7f'
down_revision = '749576d6afe2'
branch_labels = None
depends_on = None
# reference to this directory
directory = path.realpath(path.dirname(__file__))

with open(path.join(directory, '../upgrade_data/graph_v3.json'), 'r') as f:
    # Use this method to validate the content of an enrichment table
    validate_graph = fastjsonschema.compile(json.load(f))


def fix_sizing(graph):
    sizing = graph.get('sizing')
    if isinstance(sizing, list):
        new_sizing = dict()
        for idx, size in enumerate(sizing):
            new_sizing[size.get('name', idx)] = size
        graph['sizing'] = new_sizing
        print("Set dict sizing based on list")
        print(new_sizing)
        return True
    elif sizing and not isinstance(sizing, dict):
        print("Delete wrong value of sizing property")
        graph['backed_up_sizing_' + revision] = graph['sizing']
        del graph['sizing']
        return True
    return False


def fix_graph_trace_networks(graph):
    if not graph.get('trace_networks'):
        print("Create graph -> trace_networks cause it does not exist")
        graph['trace_networks'] = list()
        return True
    return False


def fix_graph_node_sets(graph):
    if not graph.get('node_sets'):
        print("Create graph -> node_sets cause it does not exist")
        graph['node_sets'] = dict()
        return True
    return False


def fix_graph_description(graph):
    if not graph.get('description'):
        print("Create graph -> description cause it does not exist")
        graph['description'] = "Autogenerated missing description"
        return True
    return False


def fix_graph(data):
    graph = data['graph']
    updated = False
    updated |= fix_sizing(graph)
    updated |= fix_graph_trace_networks(graph)
    updated |= fix_graph_node_sets(graph)
    updated |= fix_graph_description(graph)
    return updated


def fix_links_description(link):
    if not link.get('description'):
        print("Create link -> description cause it does not exist")
        link['description'] = "Autogenerated missing description"
        return True
    return False


def fix_links(data):
    links = data['links']
    updated = False
    for link in links:
        updated |= fix_links_description(link)
    return updated


def fix_sizing_param():
    conn = op.get_bind()
    session = Session(conn)

    t_files = table(
            'files',
            column('content_id', sa.Integer),
            column('mime_type', sa.String))

    t_files_content = table(
            'files_content',
            column('id', sa.Integer),
            column('raw_file', sa.LargeBinary),
            column('checksum_sha256', sa.Binary)
    )

    files = conn.execution_options(stream_results=True).execute(sa.select([
        t_files_content.c.id,
        t_files_content.c.raw_file
    ]).where(
            and_(
                    t_files.c.mime_type == 'vnd.lifelike.document/graph',
                    t_files.c.content_id == t_files_content.c.id
            )
    ))

    for chunk in window_chunk(files, 25):
        for id, content in chunk:
            data = json.loads(content)
            updated = False
            updated |= fix_graph(data)
            updated |= fix_links(data)

            if updated:
                validate_graph(data)
                raw_file = json.dumps(data).encode('utf-8')
                checksum_sha256 = hashlib.sha256(raw_file).digest()
                session.execute(
                        t_files_content.update().where(
                                t_files_content.c.id == id
                        ).values(
                                raw_file=raw_file,
                                checksum_sha256=checksum_sha256
                        )
                )
                session.flush()
    session.commit()


def upgrade():
    if context.get_x_argument(as_dictionary=True).get('data_migrate', None):
        data_upgrades()


def downgrade():
    pass


def data_upgrades():
    fix_sizing_param()


def data_downgrades():
    pass
